{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "43bc11d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /u/sameer/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from huggingface_hub import login\n",
    "\n",
    "login(token=\"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\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7beaf39e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/u/sameer/anaconda3/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab86545d312f4d9d9682f35f0296a5a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoTokenizer, AutoModelForCausalLM\n\u001b[1;32m      3\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcodellama/CodeLlama-34b-Instruct-hf\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m model \u001b[38;5;241m=\u001b[39m AutoModelForCausalLM\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcodellama/CodeLlama-34b-Instruct-hf\u001b[39m\u001b[38;5;124m\"\u001b[39m, cache_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/dccstor/ai4code-summ/benchmark-paper\u001b[39m\u001b[38;5;124m\"\u001b[39m, device_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py:563\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    561\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    562\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n\u001b[0;32m--> 563\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model_class\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[1;32m    564\u001b[0m         pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39mmodel_args, config\u001b[38;5;241m=\u001b[39mconfig, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mhub_kwargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    565\u001b[0m     )\n\u001b[1;32m    566\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    567\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    568\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    569\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/transformers/modeling_utils.py:3677\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3668\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype_orig \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3669\u001b[0m         torch\u001b[38;5;241m.\u001b[39mset_default_dtype(dtype_orig)\n\u001b[1;32m   3670\u001b[0m     (\n\u001b[1;32m   3671\u001b[0m         model,\n\u001b[1;32m   3672\u001b[0m         missing_keys,\n\u001b[1;32m   3673\u001b[0m         unexpected_keys,\n\u001b[1;32m   3674\u001b[0m         mismatched_keys,\n\u001b[1;32m   3675\u001b[0m         offload_index,\n\u001b[1;32m   3676\u001b[0m         error_msgs,\n\u001b[0;32m-> 3677\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_load_pretrained_model(\n\u001b[1;32m   3678\u001b[0m         model,\n\u001b[1;32m   3679\u001b[0m         state_dict,\n\u001b[1;32m   3680\u001b[0m         loaded_state_dict_keys,  \u001b[38;5;66;03m# XXX: rename?\u001b[39;00m\n\u001b[1;32m   3681\u001b[0m         resolved_archive_file,\n\u001b[1;32m   3682\u001b[0m         pretrained_model_name_or_path,\n\u001b[1;32m   3683\u001b[0m         ignore_mismatched_sizes\u001b[38;5;241m=\u001b[39mignore_mismatched_sizes,\n\u001b[1;32m   3684\u001b[0m         sharded_metadata\u001b[38;5;241m=\u001b[39msharded_metadata,\n\u001b[1;32m   3685\u001b[0m         _fast_init\u001b[38;5;241m=\u001b[39m_fast_init,\n\u001b[1;32m   3686\u001b[0m         low_cpu_mem_usage\u001b[38;5;241m=\u001b[39mlow_cpu_mem_usage,\n\u001b[1;32m   3687\u001b[0m         device_map\u001b[38;5;241m=\u001b[39mdevice_map,\n\u001b[1;32m   3688\u001b[0m         offload_folder\u001b[38;5;241m=\u001b[39moffload_folder,\n\u001b[1;32m   3689\u001b[0m         offload_state_dict\u001b[38;5;241m=\u001b[39moffload_state_dict,\n\u001b[1;32m   3690\u001b[0m         dtype\u001b[38;5;241m=\u001b[39mtorch_dtype,\n\u001b[1;32m   3691\u001b[0m         hf_quantizer\u001b[38;5;241m=\u001b[39mhf_quantizer,\n\u001b[1;32m   3692\u001b[0m         keep_in_fp32_modules\u001b[38;5;241m=\u001b[39mkeep_in_fp32_modules,\n\u001b[1;32m   3693\u001b[0m     )\n\u001b[1;32m   3695\u001b[0m \u001b[38;5;66;03m# make sure token embedding weights are still tied if needed\u001b[39;00m\n\u001b[1;32m   3696\u001b[0m model\u001b[38;5;241m.\u001b[39mtie_weights()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/transformers/modeling_utils.py:4104\u001b[0m, in \u001b[0;36mPreTrainedModel._load_pretrained_model\u001b[0;34m(cls, model, state_dict, loaded_keys, resolved_archive_file, pretrained_model_name_or_path, ignore_mismatched_sizes, sharded_metadata, _fast_init, low_cpu_mem_usage, device_map, offload_folder, offload_state_dict, dtype, hf_quantizer, keep_in_fp32_modules)\u001b[0m\n\u001b[1;32m   4100\u001b[0m                 set_module_tensor_to_device(\n\u001b[1;32m   4101\u001b[0m                     model_to_load, key, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m, torch\u001b[38;5;241m.\u001b[39mempty(\u001b[38;5;241m*\u001b[39mparam\u001b[38;5;241m.\u001b[39msize(), dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[1;32m   4102\u001b[0m                 )\n\u001b[1;32m   4103\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 4104\u001b[0m         new_error_msgs, offload_index, state_dict_index \u001b[38;5;241m=\u001b[39m _load_state_dict_into_meta_model(\n\u001b[1;32m   4105\u001b[0m             model_to_load,\n\u001b[1;32m   4106\u001b[0m             state_dict,\n\u001b[1;32m   4107\u001b[0m             loaded_keys,\n\u001b[1;32m   4108\u001b[0m             start_prefix,\n\u001b[1;32m   4109\u001b[0m             expected_keys,\n\u001b[1;32m   4110\u001b[0m             device_map\u001b[38;5;241m=\u001b[39mdevice_map,\n\u001b[1;32m   4111\u001b[0m             offload_folder\u001b[38;5;241m=\u001b[39moffload_folder,\n\u001b[1;32m   4112\u001b[0m             offload_index\u001b[38;5;241m=\u001b[39moffload_index,\n\u001b[1;32m   4113\u001b[0m             state_dict_folder\u001b[38;5;241m=\u001b[39mstate_dict_folder,\n\u001b[1;32m   4114\u001b[0m             state_dict_index\u001b[38;5;241m=\u001b[39mstate_dict_index,\n\u001b[1;32m   4115\u001b[0m             dtype\u001b[38;5;241m=\u001b[39mdtype,\n\u001b[1;32m   4116\u001b[0m             hf_quantizer\u001b[38;5;241m=\u001b[39mhf_quantizer,\n\u001b[1;32m   4117\u001b[0m             is_safetensors\u001b[38;5;241m=\u001b[39mis_safetensors,\n\u001b[1;32m   4118\u001b[0m             keep_in_fp32_modules\u001b[38;5;241m=\u001b[39mkeep_in_fp32_modules,\n\u001b[1;32m   4119\u001b[0m             unexpected_keys\u001b[38;5;241m=\u001b[39munexpected_keys,\n\u001b[1;32m   4120\u001b[0m         )\n\u001b[1;32m   4121\u001b[0m         error_msgs \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m new_error_msgs\n\u001b[1;32m   4122\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/transformers/modeling_utils.py:852\u001b[0m, in \u001b[0;36m_load_state_dict_into_meta_model\u001b[0;34m(model, state_dict, loaded_state_dict_keys, start_prefix, expected_keys, device_map, offload_folder, offload_index, state_dict_folder, state_dict_index, dtype, hf_quantizer, is_safetensors, keep_in_fp32_modules, unexpected_keys)\u001b[0m\n\u001b[1;32m    850\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m old_param \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    851\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 852\u001b[0m         param \u001b[38;5;241m=\u001b[39m param\u001b[38;5;241m.\u001b[39mto(old_param\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[1;32m    854\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m old_param\u001b[38;5;241m.\u001b[39mis_contiguous():\n\u001b[1;32m    855\u001b[0m         param \u001b[38;5;241m=\u001b[39m param\u001b[38;5;241m.\u001b[39mcontiguous()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"codellama/CodeLlama-34b-Instruct-hf\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"codellama/CodeLlama-34b-Instruct-hf\", cache_dir=\"/dccstor/ai4code-summ/benchmark-paper\", device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "489a5834",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.model_max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7cd9efff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon May 20 00:59:29 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.54.03              Driver Version: 535.54.03    CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA A100-SXM4-40GB          On  | 00000000:47:00.0 Off |                    0 |\n",
      "| N/A   22C    P0              55W / 400W |  33349MiB / 40960MiB |      0%   E. Process |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA A100-SXM4-40GB          On  | 00000000:4D:00.0 Off |                    0 |\n",
      "| N/A   23C    P0              57W / 400W |  35009MiB / 40960MiB |      0%   E. Process |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   2  NVIDIA A100-SXM4-40GB          On  | 00000000:8D:00.0 Off |                    0 |\n",
      "| N/A   22C    P0              55W / 400W |  35009MiB / 40960MiB |      0%   E. Process |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   3  NVIDIA A100-SXM4-40GB          On  | 00000000:C7:00.0 Off |                    0 |\n",
      "| N/A   21C    P0              55W / 400W |  28029MiB / 40960MiB |      0%   E. Process |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    0   N/A  N/A   2305353      C   /u/sameer/anaconda3/bin/python            33336MiB |\n",
      "|    1   N/A  N/A   2305353      C   /u/sameer/anaconda3/bin/python            34996MiB |\n",
      "|    2   N/A  N/A   2305353      C   /u/sameer/anaconda3/bin/python            34996MiB |\n",
      "|    3   N/A  N/A   2305353      C   /u/sameer/anaconda3/bin/python            28016MiB |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "79b1f494",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "base_path = \"./constrain-data-gen-eval/data_generator/data_generator/\"\n",
    "xml_path = \"xml_dataset_6_march/\"\n",
    "yaml_path = \"yaml_dataset_6_march/\"\n",
    "json_path = \"json_schema_dataset_28_feb/\"\n",
    "\n",
    "xml_files = os.listdir(base_path + xml_path)\n",
    "yaml_files = os.listdir(base_path + yaml_path)\n",
    "json_files = os.listdir(base_path + json_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3c41e113",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;34manaconda3\u001b[0m/                           Evaluation.ipynb      logs.txt\r\n",
      "Anaconda3-2023.09-0-Linux-x86_64.sh  inference-34B.ipynb   \u001b[01;34mold_anaconda\u001b[0m/\r\n",
      "BAM-api.ipynb                        inference-Copy.ipynb  screenlog.0\r\n",
      "\u001b[01;34mconstrain-data-gen-eval\u001b[0m/             inference.ipynb\r\n",
      "Data-statistics.ipynb                inference_job.py\r\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8aa51823",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['composite_0_50_array_i_9.json',\n",
       " 'composite_0_10_array_i_27.json',\n",
       " 'composite_0_10_object_i_14.json',\n",
       " 'string_0_100_i_15.json',\n",
       " 'number_0_10_i_29.json',\n",
       " 'number_0_50_i_40.json',\n",
       " 'composite_0_100_object_i_19.json',\n",
       " 'composite_0_50_object_i_5.json',\n",
       " 'string_0_100_i_42.json',\n",
       " 'number_0_10_i_39.json',\n",
       " 'string_0_10_i_12.json',\n",
       " 'composite_0_100_object_i_22.json',\n",
       " 'string_0_100_i_4.json',\n",
       " 'string_0_100_i_39.json',\n",
       " 'string_0_100_i_49.json',\n",
       " 'number_0_10_i_16.json',\n",
       " 'string_0_10_i_22.json',\n",
       " 'composite_0_10_object_i_2.json',\n",
       " 'composite_0_50_object_i_42.json',\n",
       " 'composite_0_50_object_i_16.json',\n",
       " 'composite_0_100_array_i_20.json',\n",
       " 'string_0_10_i_0.json',\n",
       " 'number_0_100_i_28.json',\n",
       " 'string_0_10_i_13.json',\n",
       " 'composite_0_50_array_i_46.json',\n",
       " 'composite_0_100_object_i_12.json',\n",
       " 'string_0_10_i_31.json',\n",
       " 'number_0_10_i_49.json',\n",
       " 'string_0_10_i_41.json',\n",
       " 'composite_0_100_array_i_14.json',\n",
       " 'composite_0_50_array_i_2.json',\n",
       " 'composite_0_50_object_i_19.json',\n",
       " 'composite_0_100_object_i_35.json',\n",
       " 'composite_0_10_array_i_18.json',\n",
       " 'composite_0_10_object_i_41.json',\n",
       " 'composite_0_10_object_i_44.json',\n",
       " 'composite_0_10_object_i_12.json',\n",
       " 'composite_0_10_array_i_23.json',\n",
       " 'composite_0_50_object_i_43.json',\n",
       " 'composite_0_50_array_i_5.json',\n",
       " 'composite_0_50_array_i_0.json',\n",
       " 'composite_0_10_object_i_9.json',\n",
       " 'number_0_10_i_34.json',\n",
       " 'composite_0_10_array_i_38.json',\n",
       " 'composite_0_50_array_i_16.json',\n",
       " 'string_0_50_i_39.json',\n",
       " 'number_0_50_i_22.json',\n",
       " 'composite_0_50_object_i_29.json',\n",
       " 'composite_0_50_object_i_15.json',\n",
       " 'composite_0_50_array_i_27.json',\n",
       " 'composite_0_50_object_i_12.json',\n",
       " 'string_0_10_i_18.json',\n",
       " 'composite_0_100_array_i_32.json',\n",
       " 'string_0_100_i_9.json',\n",
       " 'number_0_10_i_25.json',\n",
       " 'composite_0_50_object_i_22.json',\n",
       " 'string_0_10_i_5.json',\n",
       " 'number_0_100_i_36.json',\n",
       " 'composite_0_10_object_i_47.json',\n",
       " 'composite_0_10_array_i_29.json',\n",
       " 'string_0_100_i_2.json',\n",
       " 'string_0_10_i_20.json',\n",
       " 'composite_0_10_array_i_1.json',\n",
       " 'number_0_50_i_48.json',\n",
       " 'string_0_100_i_17.json',\n",
       " 'composite_0_10_object_i_6.json',\n",
       " 'composite_0_10_object_i_19.json',\n",
       " 'number_0_10_i_15.json',\n",
       " 'string_0_10_i_29.json',\n",
       " 'composite_0_100_array_i_33.json',\n",
       " 'composite_0_100_array_i_17.json',\n",
       " 'string_0_50_i_15.json',\n",
       " 'string_0_50_i_6.json',\n",
       " 'composite_0_10_array_i_0.json',\n",
       " 'string_0_10_i_45.json',\n",
       " 'composite_0_50_array_i_31.json',\n",
       " 'string_0_100_i_46.json',\n",
       " 'composite_0_10_object_i_33.json',\n",
       " 'string_0_10_i_23.json',\n",
       " 'composite_0_50_object_i_46.json',\n",
       " 'composite_0_10_array_i_6.json',\n",
       " 'string_0_100_i_40.json',\n",
       " 'composite_0_100_array_i_31.json',\n",
       " 'composite_0_100_object_i_46.json',\n",
       " 'number_0_10_i_38.json',\n",
       " 'number_0_10_i_19.json',\n",
       " 'number_0_10_i_42.json',\n",
       " 'composite_0_10_array_i_28.json',\n",
       " 'composite_0_100_array_i_7.json',\n",
       " 'number_0_50_i_34.json',\n",
       " 'composite_0_100_array_i_45.json',\n",
       " 'composite_0_50_object_i_2.json',\n",
       " 'number_0_50_i_15.json',\n",
       " 'number_0_10_i_45.json',\n",
       " 'composite_0_50_object_i_0.json',\n",
       " 'string_0_50_i_3.json',\n",
       " 'composite_0_10_object_i_38.json',\n",
       " 'string_0_50_i_19.json',\n",
       " 'boolean.json',\n",
       " 'number_0_10_i_5.json',\n",
       " 'composite_0_100_array_i_22.json',\n",
       " 'composite_0_100_array_i_35.json',\n",
       " 'composite_0_50_object_i_32.json',\n",
       " 'composite_0_10_object_i_40.json',\n",
       " 'composite_0_50_array_i_25.json',\n",
       " 'string_0_100_i_21.json',\n",
       " 'number_0_50_i_12.json',\n",
       " 'number_0_50_i_38.json',\n",
       " 'composite_0_10_object_i_35.json',\n",
       " 'composite_0_50_array_i_45.json',\n",
       " 'composite_0_10_object_i_18.json',\n",
       " 'composite_0_10_object_i_42.json',\n",
       " 'string_0_100_i_24.json',\n",
       " 'composite_0_100_array_i_4.json',\n",
       " 'composite_0_10_object_i_0.json',\n",
       " 'number_0_50_i_6.json',\n",
       " 'number_0_100_i_21.json',\n",
       " 'number_0_50_i_45.json',\n",
       " 'composite_0_50_object_i_35.json',\n",
       " 'composite_0_10_array_i_19.json',\n",
       " 'number_0_10_i_14.json',\n",
       " 'string_0_100_i_7.json',\n",
       " 'composite_0_10_object_i_43.json',\n",
       " 'number_0_50_i_23.json',\n",
       " 'number_0_100_i_49.json',\n",
       " 'string_0_100_i_35.json',\n",
       " 'string_0_100_i_33.json',\n",
       " 'composite_0_10_object_i_29.json',\n",
       " 'string_0_10_i_8.json',\n",
       " 'composite_0_10_object_i_16.json',\n",
       " 'composite_0_10_array_i_47.json',\n",
       " 'composite_0_10_object_i_26.json',\n",
       " 'composite_0_100_array_i_42.json',\n",
       " 'composite_0_50_array_i_12.json',\n",
       " 'composite_0_50_array_i_36.json',\n",
       " 'string_0_100_i_0.json',\n",
       " 'string_0_50_i_17.json',\n",
       " 'composite_0_100_object_i_5.json',\n",
       " 'composite_0_100_object_i_32.json',\n",
       " 'composite_0_100_array_i_5.json',\n",
       " 'composite_0_100_array_i_18.json',\n",
       " 'string_0_10_i_32.json',\n",
       " 'number_0_50_i_0.json',\n",
       " 'composite_0_100_object_i_41.json',\n",
       " 'composite_0_50_object_i_26.json',\n",
       " 'composite_0_50_array_i_11.json',\n",
       " 'composite_0_10_array_i_21.json',\n",
       " 'number_0_10_i_40.json',\n",
       " 'composite_0_10_object_i_48.json',\n",
       " 'number_0_10_i_44.json',\n",
       " 'composite_0_100_object_i_47.json',\n",
       " 'composite_0_10_array_i_3.json',\n",
       " 'string_0_10_i_19.json',\n",
       " 'string_0_50_i_33.json',\n",
       " 'composite_0_100_array_i_13.json',\n",
       " 'string_0_100_i_16.json',\n",
       " 'number_0_50_i_17.json',\n",
       " 'composite_0_100_object_i_26.json',\n",
       " 'string_0_10_i_7.json',\n",
       " 'composite_0_50_object_i_37.json',\n",
       " 'number_0_10_i_43.json',\n",
       " 'composite_0_50_object_i_45.json',\n",
       " 'number_0_50_i_28.json',\n",
       " 'composite_0_100_array_i_2.json',\n",
       " 'number_0_10_i_12.json',\n",
       " 'number_0_100_i_18.json',\n",
       " 'composite_0_10_array_i_37.json',\n",
       " 'string_0_10_i_10.json',\n",
       " 'string_0_50_i_37.json',\n",
       " 'string_0_50_i_1.json',\n",
       " 'number_0_100_i_30.json',\n",
       " 'composite_0_50_array_i_15.json',\n",
       " 'composite_0_50_array_i_40.json',\n",
       " 'composite_0_100_array_i_6.json',\n",
       " 'number_0_10_i_37.json',\n",
       " 'number_0_100_i_41.json',\n",
       " 'composite_0_100_object_i_6.json',\n",
       " 'number_0_100_i_34.json',\n",
       " 'string_0_10_i_26.json',\n",
       " 'composite_0_100_object_i_34.json',\n",
       " 'composite_0_10_array_i_12.json',\n",
       " 'composite_0_10_array_i_43.json',\n",
       " 'composite_0_100_object_i_21.json',\n",
       " 'composite_0_100_object_i_27.json',\n",
       " 'string_0_50_i_26.json',\n",
       " 'number_0_100_i_6.json',\n",
       " 'composite_0_100_array_i_8.json',\n",
       " 'composite_0_100_object_i_7.json',\n",
       " 'string_0_100_i_10.json',\n",
       " 'string_0_50_i_30.json',\n",
       " 'number_0_100_i_31.json',\n",
       " 'number_0_50_i_21.json',\n",
       " 'composite_0_100_array_i_24.json',\n",
       " 'number_0_100_i_25.json',\n",
       " 'composite_0_50_object_i_13.json',\n",
       " 'composite_0_100_object_i_42.json',\n",
       " 'composite_0_50_array_i_23.json',\n",
       " 'composite_0_100_object_i_40.json',\n",
       " 'string_0_50_i_10.json',\n",
       " 'string_0_50_i_34.json',\n",
       " 'composite_0_10_array_i_45.json',\n",
       " 'composite_0_10_object_i_8.json',\n",
       " 'composite_0_100_array_i_10.json',\n",
       " 'composite_0_100_array_i_38.json',\n",
       " 'number_0_50_i_11.json',\n",
       " 'string_0_10_i_43.json',\n",
       " 'string_0_10_i_27.json',\n",
       " 'string_0_10_i_6.json',\n",
       " 'string_0_10_i_28.json',\n",
       " 'composite_0_100_object_i_28.json',\n",
       " 'composite_0_10_object_i_31.json',\n",
       " 'number_0_50_i_32.json',\n",
       " 'composite_0_10_array_i_32.json',\n",
       " 'string_0_50_i_48.json',\n",
       " 'string_0_10_i_37.json',\n",
       " 'number_0_10_i_31.json',\n",
       " 'number_0_100_i_10.json',\n",
       " 'composite_0_10_array_i_42.json',\n",
       " 'number_0_10_i_21.json',\n",
       " 'composite_0_50_object_i_30.json',\n",
       " 'composite_0_50_array_i_30.json',\n",
       " 'number_0_50_i_25.json',\n",
       " 'composite_0_50_array_i_8.json',\n",
       " 'number_0_10_i_46.json',\n",
       " 'composite_0_10_array_i_33.json',\n",
       " 'composite_0_10_array_i_36.json',\n",
       " 'number_0_100_i_24.json',\n",
       " 'number_0_50_i_4.json',\n",
       " 'composite_0_10_array_i_46.json',\n",
       " 'number_0_50_i_24.json',\n",
       " 'string_0_50_i_11.json',\n",
       " 'number_0_10_i_8.json',\n",
       " 'number_0_50_i_44.json',\n",
       " 'string_0_50_i_41.json',\n",
       " 'number_0_10_i_26.json',\n",
       " 'number_0_100_i_16.json',\n",
       " 'number_0_50_i_3.json',\n",
       " 'string_0_50_i_29.json',\n",
       " 'composite_0_100_object_i_31.json',\n",
       " 'number_0_10_i_41.json',\n",
       " 'composite_0_10_object_i_15.json',\n",
       " 'string_0_10_i_36.json',\n",
       " 'string_0_10_i_48.json',\n",
       " 'number_0_50_i_31.json',\n",
       " 'composite_0_100_array_i_41.json',\n",
       " 'number_0_10_i_48.json',\n",
       " 'number_0_10_i_47.json',\n",
       " 'string_0_10_i_2.json',\n",
       " 'string_0_100_i_27.json',\n",
       " 'composite_0_10_object_i_46.json',\n",
       " 'composite_0_50_array_i_41.json',\n",
       " 'composite_0_10_array_i_17.json',\n",
       " 'string_0_100_i_30.json',\n",
       " 'composite_0_100_object_i_2.json',\n",
       " 'string_0_100_i_36.json',\n",
       " 'composite_0_100_object_i_8.json',\n",
       " 'number_0_100_i_47.json',\n",
       " 'number_0_10_i_11.json',\n",
       " 'string_0_50_i_40.json',\n",
       " 'composite_0_100_array_i_26.json',\n",
       " 'string_0_50_i_7.json',\n",
       " 'composite_0_100_array_i_11.json',\n",
       " 'number_0_10_i_27.json',\n",
       " 'composite_0_50_object_i_3.json',\n",
       " 'number_0_10_i_17.json',\n",
       " 'composite_0_100_array_i_46.json',\n",
       " 'number_0_100_i_12.json',\n",
       " 'number_0_50_i_30.json',\n",
       " 'composite_0_100_object_i_38.json',\n",
       " 'string_0_50_i_16.json',\n",
       " 'composite_0_10_object_i_49.json',\n",
       " 'string_0_50_i_9.json',\n",
       " 'composite_0_10_object_i_24.json',\n",
       " 'string_0_100_i_29.json',\n",
       " 'composite_0_100_object_i_36.json',\n",
       " 'number_0_10_i_9.json',\n",
       " 'composite_0_10_array_i_31.json',\n",
       " 'composite_0_100_array_i_0.json',\n",
       " 'composite_0_10_array_i_26.json',\n",
       " 'number_0_10_i_7.json',\n",
       " 'number_0_50_i_18.json',\n",
       " 'composite_0_100_array_i_49.json',\n",
       " 'composite_0_10_array_i_39.json',\n",
       " 'composite_0_50_object_i_34.json',\n",
       " 'string_0_50_i_44.json',\n",
       " 'composite_0_10_array_i_24.json',\n",
       " 'string_0_10_i_47.json',\n",
       " 'composite_0_50_object_i_6.json',\n",
       " 'string_0_100_i_1.json',\n",
       " 'number_0_10_i_3.json',\n",
       " 'number_0_50_i_14.json',\n",
       " 'composite_0_50_array_i_38.json',\n",
       " 'number_0_10_i_10.json',\n",
       " 'string_0_10_i_44.json',\n",
       " 'composite_0_50_array_i_18.json',\n",
       " 'composite_0_100_object_i_25.json',\n",
       " 'composite_0_50_array_i_32.json',\n",
       " 'number_0_100_i_5.json',\n",
       " 'composite_0_50_object_i_25.json',\n",
       " 'string_0_50_i_4.json',\n",
       " 'string_0_10_i_42.json',\n",
       " 'composite_0_10_object_i_45.json',\n",
       " 'composite_0_50_object_i_21.json',\n",
       " 'composite_0_10_array_i_7.json',\n",
       " 'composite_0_10_object_i_11.json',\n",
       " 'number_0_10_i_35.json',\n",
       " 'number_0_50_i_43.json',\n",
       " 'composite_0_50_array_i_47.json',\n",
       " 'composite_0_50_object_i_1.json',\n",
       " 'string_0_100_i_5.json',\n",
       " 'composite_0_50_object_i_4.json',\n",
       " 'string_0_10_i_11.json',\n",
       " 'string_0_50_i_32.json',\n",
       " 'number_0_50_i_37.json',\n",
       " 'composite_0_50_array_i_10.json',\n",
       " 'string_0_50_i_21.json',\n",
       " 'composite_0_10_object_i_32.json',\n",
       " 'composite_0_50_array_i_4.json',\n",
       " 'string_0_50_i_0.json',\n",
       " 'string_0_100_i_12.json',\n",
       " 'string_0_10_i_9.json',\n",
       " 'string_0_10_i_4.json',\n",
       " 'composite_0_50_array_i_44.json',\n",
       " 'number_0_100_i_40.json',\n",
       " 'string_0_100_i_3.json',\n",
       " 'number_0_50_i_7.json',\n",
       " 'string_0_50_i_36.json',\n",
       " 'composite_0_100_array_i_21.json',\n",
       " 'number_0_10_i_30.json',\n",
       " 'string_0_50_i_5.json',\n",
       " 'composite_0_10_array_i_13.json',\n",
       " 'number_0_50_i_33.json',\n",
       " 'string_0_10_i_21.json',\n",
       " 'composite_0_50_array_i_17.json',\n",
       " 'composite_0_50_object_i_41.json',\n",
       " 'number_0_10_i_6.json',\n",
       " 'string_0_10_i_38.json',\n",
       " 'composite_0_100_object_i_3.json',\n",
       " 'composite_0_50_array_i_33.json',\n",
       " 'composite_0_10_object_i_36.json',\n",
       " 'composite_0_10_array_i_16.json',\n",
       " 'string_0_100_i_19.json',\n",
       " 'string_0_100_i_20.json',\n",
       " 'number_0_50_i_42.json',\n",
       " 'composite_0_10_object_i_22.json',\n",
       " 'composite_0_100_array_i_43.json',\n",
       " 'string_0_50_i_28.json',\n",
       " 'composite_0_10_array_i_9.json',\n",
       " 'composite_0_50_array_i_21.json',\n",
       " 'number_0_10_i_22.json',\n",
       " 'composite_0_100_object_i_18.json',\n",
       " 'composite_0_100_object_i_17.json',\n",
       " 'number_0_10_i_0.json',\n",
       " 'composite_0_100_array_i_28.json',\n",
       " 'composite_0_100_array_i_40.json',\n",
       " 'composite_0_100_array_i_3.json',\n",
       " 'composite_0_10_array_i_2.json',\n",
       " 'composite_0_100_array_i_23.json',\n",
       " 'number_0_10_i_20.json',\n",
       " 'composite_0_100_object_i_0.json',\n",
       " 'number_0_100_i_39.json',\n",
       " 'composite_0_10_array_i_22.json',\n",
       " 'composite_0_10_array_i_48.json',\n",
       " 'string_0_100_i_11.json',\n",
       " 'composite_0_10_object_i_1.json',\n",
       " 'composite_0_100_object_i_48.json',\n",
       " 'composite_0_50_object_i_28.json',\n",
       " 'string_0_50_i_27.json',\n",
       " 'composite_0_50_array_i_3.json',\n",
       " 'composite_0_10_object_i_20.json',\n",
       " 'composite_0_100_array_i_37.json',\n",
       " 'string_0_10_i_49.json',\n",
       " 'string_0_10_i_3.json',\n",
       " 'composite_0_100_array_i_47.json',\n",
       " 'composite_0_50_array_i_28.json',\n",
       " 'string_0_50_i_47.json',\n",
       " 'number_0_50_i_47.json',\n",
       " 'number_0_50_i_1.json',\n",
       " 'composite_0_50_object_i_23.json',\n",
       " 'string_0_10_i_1.json',\n",
       " 'number_0_100_i_26.json',\n",
       " 'composite_0_100_array_i_29.json',\n",
       " 'composite_0_50_object_i_31.json',\n",
       " 'string_0_100_i_26.json',\n",
       " 'composite_0_10_object_i_5.json',\n",
       " 'string_0_50_i_31.json',\n",
       " 'number_0_50_i_36.json',\n",
       " 'composite_0_10_array_i_11.json',\n",
       " 'string_0_100_i_31.json',\n",
       " 'composite_0_10_object_i_10.json',\n",
       " 'composite_0_100_object_i_11.json',\n",
       " 'composite_0_50_object_i_10.json',\n",
       " 'number_0_100_i_14.json',\n",
       " 'number_0_50_i_19.json',\n",
       " 'composite_0_50_object_i_18.json',\n",
       " 'composite_0_100_object_i_4.json',\n",
       " 'composite_0_100_object_i_29.json',\n",
       " 'composite_0_50_object_i_7.json',\n",
       " 'number_0_10_i_1.json',\n",
       " 'composite_0_50_array_i_6.json',\n",
       " 'string_0_100_i_43.json',\n",
       " 'number_0_10_i_24.json',\n",
       " 'composite_0_100_array_i_34.json',\n",
       " 'composite_0_10_array_i_30.json',\n",
       " 'number_0_50_i_5.json',\n",
       " 'string_0_10_i_34.json',\n",
       " 'composite_0_10_array_i_35.json',\n",
       " 'number_0_100_i_35.json',\n",
       " 'composite_0_10_object_i_17.json',\n",
       " 'composite_0_10_array_i_14.json',\n",
       " 'number_0_100_i_33.json',\n",
       " 'number_0_50_i_46.json',\n",
       " 'string_0_50_i_14.json',\n",
       " 'composite_0_50_array_i_20.json',\n",
       " 'composite_0_50_array_i_1.json',\n",
       " 'string_0_50_i_46.json',\n",
       " 'composite_0_50_object_i_33.json',\n",
       " 'string_0_50_i_38.json',\n",
       " 'composite_0_10_array_i_40.json',\n",
       " 'string_0_10_i_35.json',\n",
       " 'composite_0_100_object_i_43.json',\n",
       " 'composite_0_50_array_i_43.json',\n",
       " 'string_0_100_i_25.json',\n",
       " 'composite_0_100_array_i_30.json',\n",
       " 'string_0_50_i_49.json',\n",
       " 'composite_0_100_object_i_49.json',\n",
       " 'string_0_100_i_34.json',\n",
       " 'string_0_50_i_20.json',\n",
       " 'string_0_100_i_8.json',\n",
       " 'number_0_50_i_41.json',\n",
       " 'composite_0_10_object_i_37.json',\n",
       " 'composite_0_100_array_i_27.json',\n",
       " 'number_0_50_i_39.json',\n",
       " 'number_0_100_i_23.json',\n",
       " 'number_0_100_i_46.json',\n",
       " 'string_0_50_i_24.json',\n",
       " 'composite_0_50_object_i_20.json',\n",
       " 'string_0_10_i_46.json',\n",
       " 'composite_0_10_array_i_44.json',\n",
       " 'composite_0_50_array_i_19.json',\n",
       " 'composite_0_10_array_i_8.json',\n",
       " 'number_0_10_i_18.json',\n",
       " 'number_0_50_i_20.json',\n",
       " 'number_0_100_i_29.json',\n",
       " 'string_0_50_i_42.json',\n",
       " 'number_0_10_i_28.json',\n",
       " 'composite_0_100_array_i_44.json',\n",
       " 'composite_0_50_object_i_14.json',\n",
       " 'number_0_10_i_2.json',\n",
       " 'number_0_50_i_2.json',\n",
       " 'string_0_100_i_28.json',\n",
       " 'composite_0_50_object_i_36.json',\n",
       " 'composite_0_50_object_i_49.json',\n",
       " 'string_0_50_i_22.json',\n",
       " 'number_0_50_i_29.json',\n",
       " 'composite_0_10_array_i_49.json',\n",
       " 'string_0_50_i_23.json',\n",
       " 'composite_0_10_array_i_4.json',\n",
       " 'number_0_100_i_1.json',\n",
       " 'string_0_100_i_48.json',\n",
       " 'string_0_100_i_22.json',\n",
       " 'number_0_50_i_16.json',\n",
       " 'composite_0_100_array_i_9.json',\n",
       " 'string_0_10_i_40.json',\n",
       " 'composite_0_10_array_i_25.json',\n",
       " 'string_0_100_i_47.json',\n",
       " 'number_0_100_i_0.json',\n",
       " 'string_0_100_i_6.json',\n",
       " 'composite_0_10_array_i_15.json',\n",
       " 'composite_0_100_object_i_44.json',\n",
       " 'string_0_10_i_14.json',\n",
       " 'string_0_50_i_12.json',\n",
       " 'number_0_100_i_44.json',\n",
       " 'string_0_10_i_16.json',\n",
       " 'string_0_100_i_44.json',\n",
       " 'number_0_50_i_9.json',\n",
       " 'number_0_100_i_38.json',\n",
       " 'string_0_100_i_23.json',\n",
       " 'composite_0_100_object_i_20.json',\n",
       " 'composite_0_50_object_i_8.json',\n",
       " 'composite_0_100_array_i_15.json',\n",
       " 'composite_0_100_object_i_39.json',\n",
       " 'composite_0_10_array_i_34.json',\n",
       " 'string_0_10_i_39.json',\n",
       " 'number_0_50_i_13.json',\n",
       " 'number_0_50_i_8.json',\n",
       " 'composite_0_100_array_i_36.json',\n",
       " 'number_0_10_i_36.json',\n",
       " 'composite_0_100_array_i_39.json',\n",
       " 'number_0_50_i_49.json',\n",
       " 'composite_0_10_object_i_4.json',\n",
       " 'number_0_100_i_13.json',\n",
       " 'composite_0_50_object_i_11.json',\n",
       " 'number_0_10_i_33.json',\n",
       " 'number_0_50_i_10.json',\n",
       " 'composite_0_10_object_i_28.json',\n",
       " 'number_0_100_i_3.json',\n",
       " 'number_0_100_i_32.json',\n",
       " 'number_0_100_i_20.json',\n",
       " 'number_0_10_i_32.json',\n",
       " 'string_0_100_i_14.json',\n",
       " 'composite_0_100_object_i_1.json',\n",
       " 'number_0_100_i_22.json',\n",
       " 'composite_0_100_array_i_48.json',\n",
       " 'string_0_100_i_37.json',\n",
       " 'number_0_100_i_37.json',\n",
       " 'number_0_100_i_9.json',\n",
       " 'string_0_50_i_43.json',\n",
       " 'composite_0_100_object_i_14.json',\n",
       " 'number_0_10_i_4.json',\n",
       " 'composite_0_10_object_i_25.json',\n",
       " 'composite_0_10_object_i_34.json',\n",
       " 'number_0_50_i_35.json',\n",
       " 'composite_0_50_object_i_47.json',\n",
       " 'composite_0_50_array_i_7.json',\n",
       " 'composite_0_50_array_i_34.json',\n",
       " 'composite_0_50_object_i_39.json',\n",
       " 'number_0_100_i_45.json',\n",
       " 'composite_0_100_object_i_10.json',\n",
       " 'composite_0_50_object_i_48.json',\n",
       " 'composite_0_100_array_i_12.json',\n",
       " 'string_0_100_i_13.json',\n",
       " 'composite_0_100_object_i_33.json',\n",
       " 'string_0_50_i_25.json',\n",
       " 'composite_0_10_object_i_23.json',\n",
       " 'composite_0_10_array_i_20.json',\n",
       " 'composite_0_50_array_i_22.json',\n",
       " 'number_0_50_i_26.json',\n",
       " 'composite_0_100_array_i_1.json',\n",
       " 'composite_0_100_object_i_23.json',\n",
       " 'number_0_100_i_15.json',\n",
       " 'string_0_10_i_15.json',\n",
       " 'composite_0_50_object_i_17.json',\n",
       " 'composite_0_50_array_i_49.json',\n",
       " 'composite_0_50_array_i_39.json',\n",
       " 'number_0_100_i_48.json',\n",
       " 'string_0_10_i_25.json',\n",
       " 'string_0_50_i_35.json',\n",
       " 'composite_0_100_object_i_9.json',\n",
       " 'composite_0_50_array_i_42.json',\n",
       " 'composite_0_100_object_i_16.json',\n",
       " 'string_0_50_i_45.json',\n",
       " 'number_0_50_i_27.json',\n",
       " 'composite_0_100_array_i_19.json',\n",
       " 'composite_0_100_object_i_45.json',\n",
       " 'string_0_10_i_17.json',\n",
       " 'composite_0_10_object_i_27.json',\n",
       " 'string_0_10_i_30.json',\n",
       " 'composite_0_50_array_i_29.json',\n",
       " 'number_0_100_i_8.json',\n",
       " 'number_0_100_i_4.json',\n",
       " 'composite_0_10_array_i_5.json',\n",
       " 'number_0_10_i_23.json',\n",
       " 'composite_0_10_object_i_7.json',\n",
       " 'composite_0_50_array_i_26.json',\n",
       " 'composite_0_50_object_i_40.json',\n",
       " 'number_0_100_i_43.json',\n",
       " 'composite_0_10_array_i_41.json',\n",
       " 'composite_0_10_object_i_21.json',\n",
       " 'string_0_50_i_2.json',\n",
       " 'composite_0_10_object_i_3.json',\n",
       " 'composite_0_100_object_i_30.json',\n",
       " 'composite_0_50_array_i_35.json',\n",
       " 'composite_0_50_array_i_37.json',\n",
       " 'string_0_100_i_32.json',\n",
       " 'composite_0_100_array_i_25.json',\n",
       " 'composite_0_10_object_i_39.json',\n",
       " 'composite_0_10_array_i_10.json',\n",
       " 'string_0_10_i_33.json',\n",
       " 'string_0_100_i_38.json',\n",
       " 'string_0_100_i_45.json',\n",
       " 'composite_0_100_array_i_16.json',\n",
       " 'number_0_10_i_13.json',\n",
       " 'composite_0_50_array_i_48.json',\n",
       " 'composite_0_50_array_i_13.json',\n",
       " 'composite_0_50_object_i_44.json',\n",
       " 'string_0_10_i_24.json',\n",
       " 'composite_0_50_object_i_27.json',\n",
       " 'string_0_100_i_41.json',\n",
       " 'composite_0_100_object_i_37.json',\n",
       " 'composite_0_10_object_i_30.json',\n",
       " 'composite_0_100_object_i_24.json',\n",
       " 'composite_0_50_object_i_38.json',\n",
       " 'number_0_100_i_27.json',\n",
       " 'composite_0_100_object_i_13.json',\n",
       " 'string_0_50_i_8.json',\n",
       " 'number_0_100_i_19.json',\n",
       " 'composite_0_50_object_i_9.json',\n",
       " 'number_0_100_i_17.json',\n",
       " 'composite_0_50_array_i_14.json',\n",
       " 'string_0_100_i_18.json',\n",
       " 'composite_0_50_object_i_24.json',\n",
       " 'number_0_100_i_42.json',\n",
       " 'composite_0_100_object_i_15.json',\n",
       " 'number_0_100_i_11.json',\n",
       " 'composite_0_50_array_i_24.json',\n",
       " 'number_0_100_i_2.json',\n",
       " 'number_0_100_i_7.json',\n",
       " 'string_0_50_i_18.json',\n",
       " 'null.json',\n",
       " 'string_0_50_i_13.json',\n",
       " 'composite_0_10_object_i_13.json']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "74019840",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                   | 0/602 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  0%|                                         | 1/602 [00:18<3:10:00, 18.97s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  0%|▏                                        | 2/602 [00:38<3:14:42, 19.47s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  0%|▏                                        | 3/602 [00:47<2:25:55, 14.62s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  1%|▎                                        | 4/602 [00:54<1:55:10, 11.56s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  1%|▎                                        | 5/602 [01:02<1:42:33, 10.31s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  1%|▍                                        | 6/602 [01:30<2:41:18, 16.24s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  1%|▍                                        | 7/602 [01:39<2:16:44, 13.79s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  1%|▌                                        | 8/602 [01:48<2:01:30, 12.27s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  1%|▌                                        | 8/602 [02:07<2:37:35, 15.92s/it]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 820.00 MiB. GPU \u0001 has a total capacity of 39.39 GiB of which 751.94 MiB is free. Including non-PyTorch memory, this process has 38.65 GiB memory in use. Of the allocated memory 37.23 GiB is allocated by PyTorch, and 953.93 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 13\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m#     prompt = \"Question:\\n \" + \"Write a JSON sample as per the JSON format schema given below.\\n\" + schema + \" \\n Answer:\\n```\"\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m#     prompt = '''This is a schema given in JSON format\\n\\nJSON format schema:\\n''' + schema + \"\\n\\nWrite a JSON sample with field values as per JSON format schema.\\nThe JSON sample must be between [SAMPLE] and [/SAMPLE] tags.\\n\\nJSON sample:\\n\"\u001b[39;00m\n\u001b[1;32m     12\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m tokenizer(prompt, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 13\u001b[0m     output \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mgenerate(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs, max_new_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m, num_beams\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)\n\u001b[1;32m     14\u001b[0m     output_text \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mbatch_decode(output, skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     15\u001b[0m     torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mempty_cache()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/transformers/generation/utils.py:1655\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1648\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   1649\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   1650\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[1;32m   1651\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   1652\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   1653\u001b[0m     )\n\u001b[1;32m   1654\u001b[0m     \u001b[38;5;66;03m# 13. run beam search\u001b[39;00m\n\u001b[0;32m-> 1655\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_beam_search(\n\u001b[1;32m   1656\u001b[0m         input_ids,\n\u001b[1;32m   1657\u001b[0m         beam_scorer,\n\u001b[1;32m   1658\u001b[0m         logits_processor\u001b[38;5;241m=\u001b[39mprepared_logits_processor,\n\u001b[1;32m   1659\u001b[0m         stopping_criteria\u001b[38;5;241m=\u001b[39mprepared_stopping_criteria,\n\u001b[1;32m   1660\u001b[0m         pad_token_id\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mpad_token_id,\n\u001b[1;32m   1661\u001b[0m         output_scores\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39moutput_scores,\n\u001b[1;32m   1662\u001b[0m         output_logits\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39moutput_logits,\n\u001b[1;32m   1663\u001b[0m         return_dict_in_generate\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mreturn_dict_in_generate,\n\u001b[1;32m   1664\u001b[0m         synced_gpus\u001b[38;5;241m=\u001b[39msynced_gpus,\n\u001b[1;32m   1665\u001b[0m         sequential\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mlow_memory,\n\u001b[1;32m   1666\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   1667\u001b[0m     )\n\u001b[1;32m   1669\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;241m==\u001b[39m GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SAMPLE:\n\u001b[1;32m   1670\u001b[0m     \u001b[38;5;66;03m# 11. prepare logits warper\u001b[39;00m\n\u001b[1;32m   1671\u001b[0m     logits_warper \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_logits_warper(generation_config)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/transformers/generation/utils.py:3171\u001b[0m, in \u001b[0;36mGenerationMixin._beam_search\u001b[0;34m(self, input_ids, beam_scorer, logits_processor, stopping_criteria, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, output_logits, return_dict_in_generate, synced_gpus, sequential, **model_kwargs)\u001b[0m\n\u001b[1;32m   3168\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m stack_model_outputs(outputs_per_sub_batch)\n\u001b[1;32m   3170\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# Unchanged original behavior\u001b[39;00m\n\u001b[0;32m-> 3171\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m(\n\u001b[1;32m   3172\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_inputs,\n\u001b[1;32m   3173\u001b[0m         return_dict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m   3174\u001b[0m         output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[1;32m   3175\u001b[0m         output_hidden_states\u001b[38;5;241m=\u001b[39moutput_hidden_states,\n\u001b[1;32m   3176\u001b[0m     )\n\u001b[1;32m   3178\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m synced_gpus \u001b[38;5;129;01mand\u001b[39;00m this_peer_finished:\n\u001b[1;32m   3179\u001b[0m     cur_len \u001b[38;5;241m=\u001b[39m cur_len \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/accelerate/hooks.py:166\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    164\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 166\u001b[0m     output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:1211\u001b[0m, in \u001b[0;36mLlamaForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m   1208\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m   1210\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m-> 1211\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(\n\u001b[1;32m   1212\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   1213\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[1;32m   1214\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[1;32m   1215\u001b[0m     past_key_values\u001b[38;5;241m=\u001b[39mpast_key_values,\n\u001b[1;32m   1216\u001b[0m     inputs_embeds\u001b[38;5;241m=\u001b[39minputs_embeds,\n\u001b[1;32m   1217\u001b[0m     use_cache\u001b[38;5;241m=\u001b[39muse_cache,\n\u001b[1;32m   1218\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[1;32m   1219\u001b[0m     output_hidden_states\u001b[38;5;241m=\u001b[39moutput_hidden_states,\n\u001b[1;32m   1220\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39mreturn_dict,\n\u001b[1;32m   1221\u001b[0m     cache_position\u001b[38;5;241m=\u001b[39mcache_position,\n\u001b[1;32m   1222\u001b[0m )\n\u001b[1;32m   1224\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1225\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpretraining_tp \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:1018\u001b[0m, in \u001b[0;36mLlamaModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m   1007\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m   1008\u001b[0m         decoder_layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m   1009\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1015\u001b[0m         cache_position,\n\u001b[1;32m   1016\u001b[0m     )\n\u001b[1;32m   1017\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1018\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m decoder_layer(\n\u001b[1;32m   1019\u001b[0m         hidden_states,\n\u001b[1;32m   1020\u001b[0m         attention_mask\u001b[38;5;241m=\u001b[39mcausal_mask,\n\u001b[1;32m   1021\u001b[0m         position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[1;32m   1022\u001b[0m         past_key_value\u001b[38;5;241m=\u001b[39mpast_key_values,\n\u001b[1;32m   1023\u001b[0m         output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[1;32m   1024\u001b[0m         use_cache\u001b[38;5;241m=\u001b[39muse_cache,\n\u001b[1;32m   1025\u001b[0m         cache_position\u001b[38;5;241m=\u001b[39mcache_position,\n\u001b[1;32m   1026\u001b[0m     )\n\u001b[1;32m   1028\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1030\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/accelerate/hooks.py:166\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    164\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 166\u001b[0m     output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:756\u001b[0m, in \u001b[0;36mLlamaDecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, **kwargs)\u001b[0m\n\u001b[1;32m    754\u001b[0m residual \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[1;32m    755\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpost_attention_layernorm(hidden_states)\n\u001b[0;32m--> 756\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmlp(hidden_states)\n\u001b[1;32m    757\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n\u001b[1;32m    759\u001b[0m outputs \u001b[38;5;241m=\u001b[39m (hidden_states,)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/accelerate/hooks.py:166\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    164\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 166\u001b[0m     output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:240\u001b[0m, in \u001b[0;36mLlamaMLP.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    238\u001b[0m     down_proj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(down_proj)\n\u001b[1;32m    239\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 240\u001b[0m     down_proj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdown_proj(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mact_fn(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgate_proj(x)) \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mup_proj(x))\n\u001b[1;32m    242\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m down_proj\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 820.00 MiB. GPU \u0001 has a total capacity of 39.39 GiB of which 751.94 MiB is free. Including non-PyTorch memory, this process has 38.65 GiB memory in use. Of the allocated memory 37.23 GiB is allocated by PyTorch, and 953.93 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "for file in tqdm(xml_files):\n",
    "    f = open(base_path+xml_path+file, \"r\")\n",
    "    schema = f.read()\n",
    "#     prompt = '''[INST] Write JSON sample with field values as per the schema given in JSON format below.\\nThe JSON sample must be between [SAMPLE] and [/SAMPLE] tags.''' + \"\\n\\n\" + schema + \"\\n[/INST]\\n[SAMPLE]\\n\"\n",
    "    prompt = '''[INST] Write JSON sample with field values as per the schema given in JSON format below.\\nThe JSON sample must be between [SAMPLE] and [/SAMPLE] tags.\\n\\n''' + schema + '''[/INST]\\n[SAMPLE]\\n```\\n'''\n",
    "\n",
    "#     prompt = \"Question:\\n \" + \"Write a JSON sample as per the JSON format schema given below.\\n\" + schema + \" \\n Answer:\\n```\"\n",
    "#     prompt = '''This is a schema given in JSON format\\n\\nJSON format schema:\\n''' + schema + \"\\n\\nWrite a JSON sample with field values as per JSON format schema.\\nThe JSON sample must be between [SAMPLE] and [/SAMPLE] tags.\\n\\nJSON sample:\\n\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    output = model.generate(**inputs, max_new_tokens=1000, num_beams=3)\n",
    "    output_text = tokenizer.batch_decode(output, skip_special_tokens=True)[0]\n",
    "    torch.cuda.empty_cache()\n",
    "    f_opt = open(\"constrain-data-gen-eval/data_generator/data_generator/xml_outputs/codellama/34B/prompt_2/\" + file, \"w\")\n",
    "    f_opt.write(output_text)\n",
    "    f_opt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d3318586",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1d3aa50d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question:\n",
      " Write a JSON sample as per the JSON format schema given below.\n",
      "{\n",
      "    \"type\": \"array\",\n",
      "    \"contains\": {\n",
      "        \"type\": \"number\",\n",
      "        \"multipleOf\": 47.07\n",
      "    },\n",
      "    \"maxContains\": 100\n",
      "} \n",
      " Answer:\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "schema = '''{\n",
    "    \"type\": \"array\",\n",
    "    \"contains\": {\n",
    "        \"type\": \"number\",\n",
    "        \"multipleOf\": 47.07\n",
    "    },\n",
    "    \"maxContains\": 100\n",
    "}'''\n",
    "\n",
    "prompt = \"Question:\\n \" + \"Write a JSON sample as per the JSON format schema given below.\\n\" + schema + \" \\n Answer:\\n```\"\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c171a65e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<?xml version=\"1.0\" ?>\n",
      "<all>\n",
      "\t<type type=\"str\">array</type>\n",
      "\t<items type=\"dict\">\n",
      "\t\t<type type=\"str\">object</type>\n",
      "\t\t<properties type=\"dict\">\n",
      "\t\t\t<pyrobitumen type=\"dict\">\n",
      "\t\t\t\t<type type=\"str\">number</type>\n",
      "\t\t\t</pyrobitumen>\n",
      "\t\t\t<heretication type=\"dict\">\n",
      "\t\t\t\t<type type=\"str\">boolean</type>\n",
      "\t\t\t</heretication>\n",
      "\t\t\t<precipices type=\"dict\">\n",
      "\t\t\t\t<type type=\"str\">string</type>\n",
      "\t\t\t</precipices>\n",
      "\t\t\t<solvement type=\"dict\">\n",
      "\t\t\t\t<type type=\"str\">null</type>\n",
      "\t\t\t</solvement>\n",
      "\t\t\t<unblamably type=\"dict\"/>\n",
      "\t\t\t<spastics type=\"dict\">\n",
      "\t\t\t\t<type type=\"str\">boolean</type>\n",
      "\t\t\t</spastics>\n",
      "\t\t\t<hoosh type=\"dict\"/>\n",
      "\t\t\t<unsplit type=\"dict\">\n",
      "\t\t\t\t<type type=\"str\">boolean</type>\n",
      "\t\t\t</unsplit>\n",
      "\t\t\t<dermatophytic type=\"dict\">\n",
      "\t\t\t\t<type type=\"str\">string</type>\n",
      "\t\t\t\t<format type=\"str\">time</format>\n",
      "\t\t\t</dermatophytic>\n",
      "\t\t\t<heartening type=\"dict\">\n",
      "\t\t\t\t<type type=\"str\">null</type>\n",
      "\t\t\t</heartening>\n",
      "\t\t\t<empyromancy type=\"dict\"/>\n",
      "\t\t\t<nostomanic type=\"dict\">\n",
      "\t\t\t\t<type type=\"str\">null</type>\n",
      "\t\t\t</nostomanic>\n",
      "\t\t\t<retund type=\"dict\"/>\n",
      "\t\t\t<upperclassman type=\"dict\">\n",
      "\t\t\t\t<type type=\"str\">null</type>\n",
      "\t\t\t</upperclassman>\n",
      "\t\t\t<dethronable type=\"dict\"/>\n",
      "\t\t\t<geisothermal type=\"dict\">\n",
      "\t\t\t\t<type type=\"str\">null</type>\n",
      "\t\t\t</geisothermal>\n",
      "\t\t\t<autoinhibited type=\"dict\">\n",
      "\t\t\t\t<type type=\"str\">string</type>\n",
      "\t\t\t</autoinhibited>\n",
      "\t\t\t<petrochemical type=\"dict\"/>\n",
      "\t\t\t<importunes type=\"dict\">\n",
      "\t\t\t\t<type type=\"str\">null</type>\n",
      "\t\t\t</importunes>\n",
      "\t\t\t<msl type=\"dict\"/>\n",
      "\t\t\t<hemianopic type=\"dict\">\n",
      "\t\t\t\t<type type=\"str\">string</type>\n",
      "\t\t\t</hemianopic>\n",
      "\t\t\t<procerity type=\"dict\">\n",
      "\t\t\t\t<type type=\"str\">null</type>\n",
      "\t\t\t</procerity>\n",
      "\t\t\t<microlepidopter type=\"dict\">\n",
      "\t\t\t\t<type type=\"str\">boolean</type>\n",
      "\t\t\t</microlepidopter>\n",
      "\t\t\t<pullalue type=\"dict\"/>\n",
      "\t\t\t<haloa type=\"dict\">\n",
      "\t\t\t\t<type type=\"str\">string</type>\n",
      "\t\t\t</haloa>\n",
      "\t\t\t<sipers type=\"dict\"/>\n",
      "\t\t\t<brache type=\"dict\"/>\n",
      "\t\t\t<nonactuality type=\"dict\"/>\n",
      "\t\t\t<indamins type=\"dict\">\n",
      "\t\t\t\t<type type=\"str\">string</type>\n",
      "\t\t\t\t<format type=\"str\">relative-json-pointer</format>\n",
      "\t\t\t</indamins>\n",
      "\t\t\t<sympathised type=\"dict\">\n",
      "\t\t\t\t<type type=\"str\">null</type>\n",
      "\t\t\t</sympathised>\n",
      "\t\t\t<iterative type=\"dict\">\n",
      "\t\t\t\t<type type=\"str\">boolean</type>\n",
      "\t\t\t</iterative>\n",
      "\t\t\t<clumsiest type=\"dict\"/>\n",
      "\t\t\t<biggity type=\"dict\"/>\n",
      "\t\t\t<desiccant type=\"dict\">\n",
      "\t\t\t\t<type type=\"str\">string</type>\n",
      "\t\t\t</desiccant>\n",
      "\t\t\t<hundredpenny type=\"dict\">\n",
      "\t\t\t\t<type type=\"str\">string</type>\n",
      "\t\t\t</hundredpenny>\n",
      "\t\t\t<siphoneae type=\"dict\">\n",
      "\t\t\t\t<type type=\"str\">null</type>\n",
      "\t\t\t</siphoneae>\n",
      "\t\t\t<jough type=\"dict\">\n",
      "\t\t\t\t<type type=\"str\">boolean</type>\n",
      "\t\t\t</jough>\n",
      "\t\t\t<sexradiate type=\"dict\">\n",
      "\t\t\t\t<type type=\"str\">boolean</type>\n",
      "\t\t\t</sexradiate>\n",
      "\t\t\t<subtepidness type=\"dict\">\n",
      "\t\t\t\t<type type=\"str\">string</type>\n",
      "\t\t\t</subtepidness>\n",
      "\t\t\t<flavorous type=\"dict\">\n",
      "\t\t\t\t<type type=\"str\">string</type>\n",
      "\t\t\t</flavorous>\n",
      "\t\t\t<rigidist type=\"dict\">\n",
      "\t\t\t\t<type type=\"str\">boolean</type>\n",
      "\t\t\t</rigidist>\n",
      "\t\t\t<rousseauist type=\"dict\">\n",
      "\t\t\t\t<type type=\"str\">string</type>\n",
      "\t\t\t\t<format type=\"str\">duration</format>\n",
      "\t\t\t\t<maxLength type=\"int\">59</maxLength>\n",
      "\t\t\t</rousseauist>\n",
      "\t\t\t<ridgepoled type=\"dict\"/>\n",
      "\t\t\t<physiologic type=\"dict\">\n",
      "\t\t\t\t<type type=\"str\">string</type>\n",
      "\t\t\t</physiologic>\n",
      "\t\t\t<mujtahid type=\"dict\"/>\n",
      "\t\t\t<discriminativeness type=\"dict\">\n",
      "\t\t\t\t<type type=\"str\">boolean</type>\n",
      "\t\t\t</discriminativeness>\n",
      "\t\t\t<greets type=\"dict\">\n",
      "\t\t\t\t<type type=\"str\">number</type>\n",
      "\t\t\t</greets>\n",
      "\t\t\t<homothallic type=\"dict\">\n",
      "\t\t\t\t<type type=\"str\">boolean</type>\n",
      "\t\t\t</homothallic>\n",
      "\t\t\t<megalocardia type=\"dict\">\n",
      "\t\t\t\t<type type=\"str\">null</type>\n",
      "\t\t\t</megalocardia>\n",
      "\t\t\t<dishwashings type=\"dict\"/>\n",
      "\t\t\t<unhygienic type=\"dict\">\n",
      "\t\t\t\t<type type=\"str\">null</type>\n",
      "\t\t\t</unhygienic>\n",
      "\t\t\t<fusobteria type=\"dict\">\n",
      "\t\t\t\t<type type=\"str\">null</type>\n",
      "\t\t\t</fusobteria>\n",
      "\t\t\t<praelects type=\"dict\">\n",
      "\t\t\t\t<type type=\"str\">boolean</type>\n",
      "\t\t\t</praelects>\n",
      "\t\t\t<retrogressive type=\"dict\">\n",
      "\t\t\t\t<type type=\"str\">null</type>\n",
      "\t\t\t</retrogressive>\n",
      "\t\t</properties>\n",
      "\t\t<additionalProperties type=\"bool\">true</additionalProperties>\n",
      "\t</items>\n",
      "</all>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "file = \"composite_0_100_array_i_0.xml\"\n",
    "f = open(base_path+xml_path+file, \"r\")\n",
    "schema = f.read()\n",
    "print(schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3ad72291",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INST] Write JSON sample with field values as per the schema given in JSON format below.\n",
      "The JSON sample must be between [SAMPLE] and [/SAMPLE] tags.\n",
      "\n",
      "<?xml version=\"1.0\" ?>\n",
      "<all>\n",
      "\t<type type=\"str\">array</type>\n",
      "\t<items type=\"dict\">\n",
      "\t\t<type type=\"str\">object</type>\n",
      "\t\t<properties type=\"dict\">\n",
      "\t\t\t<pyrobitumen type=\"dict\">\n",
      "\t\t\t\t<type type=\"str\">number</type>\n",
      "\t\t\t</pyrobitumen>\n",
      "\t\t\t<heretication type=\"dict\">\n",
      "\t\t\t\t<type type=\"str\">boolean</type>\n",
      "\t\t\t</heretication>\n",
      "\t\t\t<precipices type=\"dict\">\n",
      "\t\t\t\t<type type=\"str\">string</type>\n",
      "\t\t\t</precipices>\n",
      "\t\t\t<solvement type=\"dict\">\n",
      "\t\t\t\t<type type=\"str\">null</type>\n",
      "\t\t\t</solvement>\n",
      "\t\t\t<unblamably type=\"dict\"/>\n",
      "\t\t\t<spastics type=\"dict\">\n",
      "\t\t\t\t<type type=\"str\">boolean</type>\n",
      "\t\t\t</spastics>\n",
      "\t\t\t<hoosh type=\"dict\"/>\n",
      "\t\t\t<unsplit type=\"dict\">\n",
      "\t\t\t\t<type type=\"str\">boolean</type>\n",
      "\t\t\t</unsplit>\n",
      "\t\t\t<dermatophytic type=\"dict\">\n",
      "\t\t\t\t<type type=\"str\">string</type>\n",
      "\t\t\t\t<format type=\"str\">time</format>\n",
      "\t\t\t</dermatophytic>\n",
      "\t\t\t<heartening type=\"dict\">\n",
      "\t\t\t\t<type type=\"str\">null</type>\n",
      "\t\t\t</heartening>\n",
      "\t\t\t<empyromancy type=\"dict\"/>\n",
      "\t\t\t<nostomanic type=\"dict\">\n",
      "\t\t\t\t<type type=\"str\">null</type>\n",
      "\t\t\t</nostomanic>\n",
      "\t\t\t<retund type=\"dict\"/>\n",
      "\t\t\t<upperclassman type=\"dict\">\n",
      "\t\t\t\t<type type=\"str\">null</type>\n",
      "\t\t\t</upperclassman>\n",
      "\t\t\t<dethronable type=\"dict\"/>\n",
      "\t\t\t<geisothermal type=\"dict\">\n",
      "\t\t\t\t<type type=\"str\">null</type>\n",
      "\t\t\t</geisothermal>\n",
      "\t\t\t<autoinhibited type=\"dict\">\n",
      "\t\t\t\t<type type=\"str\">string</type>\n",
      "\t\t\t</autoinhibited>\n",
      "\t\t\t<petrochemical type=\"dict\"/>\n",
      "\t\t\t<importunes type=\"dict\">\n",
      "\t\t\t\t<type type=\"str\">null</type>\n",
      "\t\t\t</importunes>\n",
      "\t\t\t<msl type=\"dict\"/>\n",
      "\t\t\t<hemianopic type=\"dict\">\n",
      "\t\t\t\t<type type=\"str\">string</type>\n",
      "\t\t\t</hemianopic>\n",
      "\t\t\t<procerity type=\"dict\">\n",
      "\t\t\t\t<type type=\"str\">null</type>\n",
      "\t\t\t</procerity>\n",
      "\t\t\t<microlepidopter type=\"dict\">\n",
      "\t\t\t\t<type type=\"str\">boolean</type>\n",
      "\t\t\t</microlepidopter>\n",
      "\t\t\t<pullalue type=\"dict\"/>\n",
      "\t\t\t<haloa type=\"dict\">\n",
      "\t\t\t\t<type type=\"str\">string</type>\n",
      "\t\t\t</haloa>\n",
      "\t\t\t<sipers type=\"dict\"/>\n",
      "\t\t\t<brache type=\"dict\"/>\n",
      "\t\t\t<nonactuality type=\"dict\"/>\n",
      "\t\t\t<indamins type=\"dict\">\n",
      "\t\t\t\t<type type=\"str\">string</type>\n",
      "\t\t\t\t<format type=\"str\">relative-json-pointer</format>\n",
      "\t\t\t</indamins>\n",
      "\t\t\t<sympathised type=\"dict\">\n",
      "\t\t\t\t<type type=\"str\">null</type>\n",
      "\t\t\t</sympathised>\n",
      "\t\t\t<iterative type=\"dict\">\n",
      "\t\t\t\t<type type=\"str\">boolean</type>\n",
      "\t\t\t</iterative>\n",
      "\t\t\t<clumsiest type=\"dict\"/>\n",
      "\t\t\t<biggity type=\"dict\"/>\n",
      "\t\t\t<desiccant type=\"dict\">\n",
      "\t\t\t\t<type type=\"str\">string</type>\n",
      "\t\t\t</desiccant>\n",
      "\t\t\t<hundredpenny type=\"dict\">\n",
      "\t\t\t\t<type type=\"str\">string</type>\n",
      "\t\t\t</hundredpenny>\n",
      "\t\t\t<siphoneae type=\"dict\">\n",
      "\t\t\t\t<type type=\"str\">null</type>\n",
      "\t\t\t</siphoneae>\n",
      "\t\t\t<jough type=\"dict\">\n",
      "\t\t\t\t<type type=\"str\">boolean</type>\n",
      "\t\t\t</jough>\n",
      "\t\t\t<sexradiate type=\"dict\">\n",
      "\t\t\t\t<type type=\"str\">boolean</type>\n",
      "\t\t\t</sexradiate>\n",
      "\t\t\t<subtepidness type=\"dict\">\n",
      "\t\t\t\t<type type=\"str\">string</type>\n",
      "\t\t\t</subtepidness>\n",
      "\t\t\t<flavorous type=\"dict\">\n",
      "\t\t\t\t<type type=\"str\">string</type>\n",
      "\t\t\t</flavorous>\n",
      "\t\t\t<rigidist type=\"dict\">\n",
      "\t\t\t\t<type type=\"str\">boolean</type>\n",
      "\t\t\t</rigidist>\n",
      "\t\t\t<rousseauist type=\"dict\">\n",
      "\t\t\t\t<type type=\"str\">string</type>\n",
      "\t\t\t\t<format type=\"str\">duration</format>\n",
      "\t\t\t\t<maxLength type=\"int\">59</maxLength>\n",
      "\t\t\t</rousseauist>\n",
      "\t\t\t<ridgepoled type=\"dict\"/>\n",
      "\t\t\t<physiologic type=\"dict\">\n",
      "\t\t\t\t<type type=\"str\">string</type>\n",
      "\t\t\t</physiologic>\n",
      "\t\t\t<mujtahid type=\"dict\"/>\n",
      "\t\t\t<discriminativeness type=\"dict\">\n",
      "\t\t\t\t<type type=\"str\">boolean</type>\n",
      "\t\t\t</discriminativeness>\n",
      "\t\t\t<greets type=\"dict\">\n",
      "\t\t\t\t<type type=\"str\">number</type>\n",
      "\t\t\t</greets>\n",
      "\t\t\t<homothallic type=\"dict\">\n",
      "\t\t\t\t<type type=\"str\">boolean</type>\n",
      "\t\t\t</homothallic>\n",
      "\t\t\t<megalocardia type=\"dict\">\n",
      "\t\t\t\t<type type=\"str\">null</type>\n",
      "\t\t\t</megalocardia>\n",
      "\t\t\t<dishwashings type=\"dict\"/>\n",
      "\t\t\t<unhygienic type=\"dict\">\n",
      "\t\t\t\t<type type=\"str\">null</type>\n",
      "\t\t\t</unhygienic>\n",
      "\t\t\t<fusobteria type=\"dict\">\n",
      "\t\t\t\t<type type=\"str\">null</type>\n",
      "\t\t\t</fusobteria>\n",
      "\t\t\t<praelects type=\"dict\">\n",
      "\t\t\t\t<type type=\"str\">boolean</type>\n",
      "\t\t\t</praelects>\n",
      "\t\t\t<retrogressive type=\"dict\">\n",
      "\t\t\t\t<type type=\"str\">null</type>\n",
      "\t\t\t</retrogressive>\n",
      "\t\t</properties>\n",
      "\t\t<additionalProperties type=\"bool\">true</additionalProperties>\n",
      "\t</items>\n",
      "</all>\n",
      "[/INST]\n",
      "[SAMPLE]\n",
      "```\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = '''[INST] Write JSON sample with field values as per the schema given in JSON format below.\\nThe JSON sample must be between [SAMPLE] and [/SAMPLE] tags.\\n\\n''' + schema + '''[/INST]\\n[SAMPLE]\\n```\\n'''\n",
    "print(prompt)\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "output = model.generate(**inputs, max_new_tokens=1000, num_beams=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1469252b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "# inputs = tokenizer(\n",
    "# '''Question:\n",
    "# Write a JSON sample as per the JSON format schema given below.\n",
    "\n",
    "# {\n",
    "#     \"type\": \"object\",\n",
    "#     \"properties\": {\n",
    "#         \"convulsional\": {\n",
    "#             \"type\": \"boolean\"\n",
    "#         },\n",
    "#         \"bricklining\": {\n",
    "#             \"type\": \"string\"\n",
    "#         },\n",
    "#         \"conquerers\": {\n",
    "#             \"type\": \"boolean\"\n",
    "#         },\n",
    "#         \"untiring\": {\n",
    "#             \"type\": \"number\"\n",
    "#         },\n",
    "#         \"relighten\": {\n",
    "#             \"type\": \"number\",\n",
    "#             \"minimum\": 6.716611460406896\n",
    "#         },\n",
    "#         \"fulimart\": {\n",
    "#             \"type\": \"null\"\n",
    "#         },\n",
    "#         \"stibium\": {\n",
    "#             \"type\": \"string\"\n",
    "#         },\n",
    "#         \"odontaspididae\": {\n",
    "#             \"type\": \"string\"\n",
    "#         },\n",
    "#         \"maladminister\": {\n",
    "#             \"type\": \"number\"\n",
    "#         },\n",
    "#         \"overreservedness\": {\n",
    "#             \"type\": \"boolean\"\n",
    "#         },\n",
    "#         \"deciduously\": {\n",
    "#             \"type\": \"string\"\n",
    "#         },\n",
    "#         \"seership\": {\n",
    "#             \"type\": \"null\"\n",
    "#         },\n",
    "#         \"backstabbed\": {\n",
    "#             \"type\": \"boolean\"\n",
    "#         },\n",
    "#         \"myxoflagellate\": {\n",
    "#             \"type\": \"string\"\n",
    "#         },\n",
    "#         \"burdie\": {\n",
    "#             \"type\": \"boolean\"\n",
    "#         },\n",
    "#         \"monographic\": {},\n",
    "#         \"alerion\": {\n",
    "#             \"type\": \"boolean\"\n",
    "#         },\n",
    "#         \"intervary\": {\n",
    "#             \"type\": \"null\"\n",
    "#         },\n",
    "#         \"multimedia\": {},\n",
    "#         \"walkouts\": {\n",
    "#             \"type\": \"boolean\"\n",
    "#         },\n",
    "#         \"peroxisome\": {\n",
    "#             \"type\": \"string\"\n",
    "#         },\n",
    "#         \"dauntingness\": {},\n",
    "#         \"midyears\": {\n",
    "#             \"type\": \"boolean\"\n",
    "#         },\n",
    "#         \"morrows\": {},\n",
    "#         \"exflagellate\": {\n",
    "#             \"type\": \"null\"\n",
    "#         },\n",
    "#         \"sheeppen\": {\n",
    "#             \"type\": \"null\"\n",
    "#         },\n",
    "#         \"explants\": {\n",
    "#             \"type\": \"number\"\n",
    "#         },\n",
    "#         \"cantilevering\": {\n",
    "#             \"type\": \"null\"\n",
    "#         },\n",
    "#         \"gimbaled\": {\n",
    "#             \"type\": \"null\"\n",
    "#         },\n",
    "#         \"virgilism\": {\n",
    "#             \"type\": \"number\",\n",
    "#             \"multipleOf\": 63.92,\n",
    "#             \"exclusiveMinimum\": 3.318385041573113,\n",
    "#             \"maximum\": 93.08874716853992\n",
    "#         }\n",
    "#     },\n",
    "#     \"additionalProperties\": false\n",
    "# }\n",
    "\n",
    "# JSON sample:\n",
    "# ```\n",
    "# ''', return_tensors=\"pt\")\n",
    "\n",
    "prompt = '''[INST]\n",
    "Write JSON sample with field values as per the schema given in XML format below\n",
    "\n",
    "\n",
    "[/INST]'''\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "output = model.generate(**inputs, max_new_tokens=1000, num_beams=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "6b7309aa",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'demofile2.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[60], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdemofile2.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(f\u001b[38;5;241m.\u001b[39mread())\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/IPython/core/interactiveshell.py:286\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    279\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    280\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    281\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    282\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    283\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    284\u001b[0m     )\n\u001b[0;32m--> 286\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m io_open(file, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'demofile2.txt'"
     ]
    }
   ],
   "source": [
    "f = open(\"demofile2.txt\", \"r\")\n",
    "print(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "08101c47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INST] Write JSON sample with field values as per the schema given in JSON format below.\n",
      "The JSON sample must be between [SAMPLE] and [/SAMPLE] tags.\n",
      "\n",
      "<?xml version=\"1.0\" ?>\n",
      "<all>\n",
      "\t<type type=\"str\">array</type>\n",
      "\t<items type=\"dict\">\n",
      "\t\t<type type=\"str\">object</type>\n",
      "\t\t<properties type=\"dict\">\n",
      "\t\t\t<pyrobitumen type=\"dict\">\n",
      "\t\t\t\t<type type=\"str\">number</type>\n",
      "\t\t\t</pyrobitumen>\n",
      "\t\t\t<heretication type=\"dict\">\n",
      "\t\t\t\t<type type=\"str\">boolean</type>\n",
      "\t\t\t</heretication>\n",
      "\t\t\t<precipices type=\"dict\">\n",
      "\t\t\t\t<type type=\"str\">string</type>\n",
      "\t\t\t</precipices>\n",
      "\t\t\t<solvement type=\"dict\">\n",
      "\t\t\t\t<type type=\"str\">null</type>\n",
      "\t\t\t</solvement>\n",
      "\t\t\t<unblamably type=\"dict\"/>\n",
      "\t\t\t<spastics type=\"dict\">\n",
      "\t\t\t\t<type type=\"str\">boolean</type>\n",
      "\t\t\t</spastics>\n",
      "\t\t\t<hoosh type=\"dict\"/>\n",
      "\t\t\t<unsplit type=\"dict\">\n",
      "\t\t\t\t<type type=\"str\">boolean</type>\n",
      "\t\t\t</unsplit>\n",
      "\t\t\t<dermatophytic type=\"dict\">\n",
      "\t\t\t\t<type type=\"str\">string</type>\n",
      "\t\t\t\t<format type=\"str\">time</format>\n",
      "\t\t\t</dermatophytic>\n",
      "\t\t\t<heartening type=\"dict\">\n",
      "\t\t\t\t<type type=\"str\">null</type>\n",
      "\t\t\t</heartening>\n",
      "\t\t\t<empyromancy type=\"dict\"/>\n",
      "\t\t\t<nostomanic type=\"dict\">\n",
      "\t\t\t\t<type type=\"str\">null</type>\n",
      "\t\t\t</nostomanic>\n",
      "\t\t\t<retund type=\"dict\"/>\n",
      "\t\t\t<upperclassman type=\"dict\">\n",
      "\t\t\t\t<type type=\"str\">null</type>\n",
      "\t\t\t</upperclassman>\n",
      "\t\t\t<dethronable type=\"dict\"/>\n",
      "\t\t\t<geisothermal type=\"dict\">\n",
      "\t\t\t\t<type type=\"str\">null</type>\n",
      "\t\t\t</geisothermal>\n",
      "\t\t\t<autoinhibited type=\"dict\">\n",
      "\t\t\t\t<type type=\"str\">string</type>\n",
      "\t\t\t</autoinhibited>\n",
      "\t\t\t<petrochemical type=\"dict\"/>\n",
      "\t\t\t<importunes type=\"dict\">\n",
      "\t\t\t\t<type type=\"str\">null</type>\n",
      "\t\t\t</importunes>\n",
      "\t\t\t<msl type=\"dict\"/>\n",
      "\t\t\t<hemianopic type=\"dict\">\n",
      "\t\t\t\t<type type=\"str\">string</type>\n",
      "\t\t\t</hemianopic>\n",
      "\t\t\t<procerity type=\"dict\">\n",
      "\t\t\t\t<type type=\"str\">null</type>\n",
      "\t\t\t</procerity>\n",
      "\t\t\t<microlepidopter type=\"dict\">\n",
      "\t\t\t\t<type type=\"str\">boolean</type>\n",
      "\t\t\t</microlepidopter>\n",
      "\t\t\t<pullalue type=\"dict\"/>\n",
      "\t\t\t<haloa type=\"dict\">\n",
      "\t\t\t\t<type type=\"str\">string</type>\n",
      "\t\t\t</haloa>\n",
      "\t\t\t<sipers type=\"dict\"/>\n",
      "\t\t\t<brache type=\"dict\"/>\n",
      "\t\t\t<nonactuality type=\"dict\"/>\n",
      "\t\t\t<indamins type=\"dict\">\n",
      "\t\t\t\t<type type=\"str\">string</type>\n",
      "\t\t\t\t<format type=\"str\">relative-json-pointer</format>\n",
      "\t\t\t</indamins>\n",
      "\t\t\t<sympathised type=\"dict\">\n",
      "\t\t\t\t<type type=\"str\">null</type>\n",
      "\t\t\t</sympathised>\n",
      "\t\t\t<iterative type=\"dict\">\n",
      "\t\t\t\t<type type=\"str\">boolean</type>\n",
      "\t\t\t</iterative>\n",
      "\t\t\t<clumsiest type=\"dict\"/>\n",
      "\t\t\t<biggity type=\"dict\"/>\n",
      "\t\t\t<desiccant type=\"dict\">\n",
      "\t\t\t\t<type type=\"str\">string</type>\n",
      "\t\t\t</desiccant>\n",
      "\t\t\t<hundredpenny type=\"dict\">\n",
      "\t\t\t\t<type type=\"str\">string</type>\n",
      "\t\t\t</hundredpenny>\n",
      "\t\t\t<siphoneae type=\"dict\">\n",
      "\t\t\t\t<type type=\"str\">null</type>\n",
      "\t\t\t</siphoneae>\n",
      "\t\t\t<jough type=\"dict\">\n",
      "\t\t\t\t<type type=\"str\">boolean</type>\n",
      "\t\t\t</jough>\n",
      "\t\t\t<sexradiate type=\"dict\">\n",
      "\t\t\t\t<type type=\"str\">boolean</type>\n",
      "\t\t\t</sexradiate>\n",
      "\t\t\t<subtepidness type=\"dict\">\n",
      "\t\t\t\t<type type=\"str\">string</type>\n",
      "\t\t\t</subtepidness>\n",
      "\t\t\t<flavorous type=\"dict\">\n",
      "\t\t\t\t<type type=\"str\">string</type>\n",
      "\t\t\t</flavorous>\n",
      "\t\t\t<rigidist type=\"dict\">\n",
      "\t\t\t\t<type type=\"str\">boolean</type>\n",
      "\t\t\t</rigidist>\n",
      "\t\t\t<rousseauist type=\"dict\">\n",
      "\t\t\t\t<type type=\"str\">string</type>\n",
      "\t\t\t\t<format type=\"str\">duration</format>\n",
      "\t\t\t\t<maxLength type=\"int\">59</maxLength>\n",
      "\t\t\t</rousseauist>\n",
      "\t\t\t<ridgepoled type=\"dict\"/>\n",
      "\t\t\t<physiologic type=\"dict\">\n",
      "\t\t\t\t<type type=\"str\">string</type>\n",
      "\t\t\t</physiologic>\n",
      "\t\t\t<mujtahid type=\"dict\"/>\n",
      "\t\t\t<discriminativeness type=\"dict\">\n",
      "\t\t\t\t<type type=\"str\">boolean</type>\n",
      "\t\t\t</discriminativeness>\n",
      "\t\t\t<greets type=\"dict\">\n",
      "\t\t\t\t<type type=\"str\">number</type>\n",
      "\t\t\t</greets>\n",
      "\t\t\t<homothallic type=\"dict\">\n",
      "\t\t\t\t<type type=\"str\">boolean</type>\n",
      "\t\t\t</homothallic>\n",
      "\t\t\t<megalocardia type=\"dict\">\n",
      "\t\t\t\t<type type=\"str\">null</type>\n",
      "\t\t\t</megalocardia>\n",
      "\t\t\t<dishwashings type=\"dict\"/>\n",
      "\t\t\t<unhygienic type=\"dict\">\n",
      "\t\t\t\t<type type=\"str\">null</type>\n",
      "\t\t\t</unhygienic>\n",
      "\t\t\t<fusobteria type=\"dict\">\n",
      "\t\t\t\t<type type=\"str\">null</type>\n",
      "\t\t\t</fusobteria>\n",
      "\t\t\t<praelects type=\"dict\">\n",
      "\t\t\t\t<type type=\"str\">boolean</type>\n",
      "\t\t\t</praelects>\n",
      "\t\t\t<retrogressive type=\"dict\">\n",
      "\t\t\t\t<type type=\"str\">null</type>\n",
      "\t\t\t</retrogressive>\n",
      "\t\t</properties>\n",
      "\t\t<additionalProperties type=\"bool\">true</additionalProperties>\n",
      "\t</items>\n",
      "</all>\n",
      "[/INST]\n",
      "[SAMPLE]\n",
      "```\n",
      "{\n",
      "  \"pyrobitumen\": 123,\n",
      "  \"heretication\": true,\n",
      "  \"precipices\": \"string\",\n",
      "  \"solvement\": null,\n",
      "  \"unblamably\": {},\n",
      "  \"spastics\": true,\n",
      "  \"hoosh\": {},\n",
      "  \"unsplit\": true,\n",
      "  \"dermatophytic\": \"time\",\n",
      "  \"heartening\": null,\n",
      "  \"empyromancy\": {},\n",
      "  \"nostomanic\": null,\n",
      "  \"retund\": true,\n",
      "  \"upperclassman\": null,\n",
      "  \"dethronable\": {},\n",
      "  \"geisothermal\": null,\n",
      "  \"autoinhibited\": \"string\",\n",
      "  \"petrochemical\": {},\n",
      "  \"importunes\": null,\n",
      "  \"msl\": {},\n",
      "  \"hemianopic\": \"string\",\n",
      "  \"procerity\": null,\n",
      "  \"microlepidopter\": true,\n",
      "  \"pullalue\": {},\n",
      "  \"haloa\": \"string\",\n",
      "  \"sipers\": {},\n",
      "  \"brache\": {},\n",
      "  \"nonactuality\": {},\n",
      "  \"indamins\": \"relative-json-pointer\",\n",
      "  \"sympathised\": null,\n",
      "  \"iterative\": true,\n",
      "  \"clumsiest\": {},\n",
      "  \"biggity\": {},\n",
      "  \"desiccant\": \"string\",\n",
      "  \"hundredpenny\": \"string\",\n",
      "  \"siphoneae\": null,\n",
      "  \"jough\": true,\n",
      "  \"sexradiate\": true,\n",
      "  \"subtepidness\": \"string\",\n",
      "  \"flavorous\": \"string\",\n",
      "  \"rigidist\": true,\n",
      "  \"rousseauist\": \"duration\",\n",
      "  \"ridgepoled\": {},\n",
      "  \"physiologic\": \"string\",\n",
      "  \"mujtahid\": {},\n",
      "  \"discriminativeness\": true,\n",
      "  \"greets\": 123,\n",
      "  \"homothallic\": true,\n",
      "  \"megalocardia\": null,\n",
      "  \"dishwashings\": {},\n",
      "  \"unhygienic\": null,\n",
      "  \"fusobteria\": null,\n",
      "  \"praelects\": true,\n",
      "  \"retrogressive\": null\n",
      "}\n",
      "```\n",
      "[/SAMPLE]\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.batch_decode(output, skip_special_tokens=True)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b680ecd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
